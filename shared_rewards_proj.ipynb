{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YlWmzCCILkQf"
      },
      "source": [
        "# **Final Project: Exploring Reward Sharing Strategies for Effective Cooperative Multi-Agent Task Completion**\n",
        "\n",
        "### **Due Date**: 05/10/2025 at 11:59 PM\n",
        "\n",
        "####**Project Proposal (Graded copy)**: https://drive.google.com/file/d/1xSh_ITemfAfa_ivwwx9Q2IUzGsIaF9dD/view?usp=sharing\n",
        "#### **Final Report**: https://docs.google.com/document/d/1Lul-CaBZPpzwnsBR4dFv6vvO8a7qrXCR_R_F0lgOIqc/edit?usp=sharing\n",
        "\n",
        "####**Video Presentation**:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# WHAT TO RECORD:\n",
        "# See Final report. It's there."
      ],
      "metadata": {
        "id": "KgXjq4qS7sKQ"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xwTW7FTsL6qV"
      },
      "source": [
        "# **Introduction**\n",
        "\n",
        "Welcome to the our Final Project of CS 4756/5756. In this project, we will train multiple agents in the Simple Spread environment using MAPPO to investigate how different reward structures - individual, shared, and partially shared - affect learning dynamics and group behavior.\n",
        "\n",
        "We will use the [**Simple Spread**](https://pettingzoo.farama.org/environments/mpe/simple_spread/) environment from the [PettingZoo](https://pettingzoo.farama.org/content/basic_usage/) library for this project.\n",
        "\n",
        "\n",
        "**Note**: Our code is an adaptation of [the official MAPPO implementation](https://github.com/marlbenchmark/on-policy) to use different reward schemes."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Setup**"
      ],
      "metadata": {
        "id": "RAufoSzYq4jq"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jdRJabMtQDy0"
      },
      "source": [
        "## Mount Google Collab and Change Working Directory\n",
        "Mount the google drive and change working directory to personal google drive."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# mount google collab\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Change to personal drive\n",
        "os.chdir('/content/drive/MyDrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uO9917WLqofC",
        "outputId": "21b26119-00b6-40b9-a817-54e5230260db"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install Necessary Libraries"
      ],
      "metadata": {
        "id": "l5SNmTVFq8uY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "collapsed": true,
        "id": "Dl4AJ_MyQ6Nt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1216cec-d781-4a7c-9866-80455e968858"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Selecting previously unselected package libglx-dev:amd64.\n",
            "(Reading database ... 126102 files and directories currently installed.)\n",
            "Preparing to unpack .../00-libglx-dev_1.4.0-1_amd64.deb ...\n",
            "Unpacking libglx-dev:amd64 (1.4.0-1) ...\n",
            "Selecting previously unselected package libgl-dev:amd64.\n",
            "Preparing to unpack .../01-libgl-dev_1.4.0-1_amd64.deb ...\n",
            "Unpacking libgl-dev:amd64 (1.4.0-1) ...\n",
            "Selecting previously unselected package libegl-dev:amd64.\n",
            "Preparing to unpack .../02-libegl-dev_1.4.0-1_amd64.deb ...\n",
            "Unpacking libegl-dev:amd64 (1.4.0-1) ...\n",
            "Selecting previously unselected package libgles1:amd64.\n",
            "Preparing to unpack .../03-libgles1_1.4.0-1_amd64.deb ...\n",
            "Unpacking libgles1:amd64 (1.4.0-1) ...\n",
            "Selecting previously unselected package libgles-dev:amd64.\n",
            "Preparing to unpack .../04-libgles-dev_1.4.0-1_amd64.deb ...\n",
            "Unpacking libgles-dev:amd64 (1.4.0-1) ...\n",
            "Selecting previously unselected package libglvnd-core-dev:amd64.\n",
            "Preparing to unpack .../05-libglvnd-core-dev_1.4.0-1_amd64.deb ...\n",
            "Unpacking libglvnd-core-dev:amd64 (1.4.0-1) ...\n",
            "Selecting previously unselected package libopengl-dev:amd64.\n",
            "Preparing to unpack .../06-libopengl-dev_1.4.0-1_amd64.deb ...\n",
            "Unpacking libopengl-dev:amd64 (1.4.0-1) ...\n",
            "Selecting previously unselected package libglvnd-dev:amd64.\n",
            "Preparing to unpack .../07-libglvnd-dev_1.4.0-1_amd64.deb ...\n",
            "Unpacking libglvnd-dev:amd64 (1.4.0-1) ...\n",
            "Selecting previously unselected package libgl1-mesa-dev:amd64.\n",
            "Preparing to unpack .../08-libgl1-mesa-dev_23.2.1-1ubuntu3.1~22.04.3_amd64.deb ...\n",
            "Unpacking libgl1-mesa-dev:amd64 (23.2.1-1ubuntu3.1~22.04.3) ...\n",
            "Selecting previously unselected package libglu1-mesa:amd64.\n",
            "Preparing to unpack .../09-libglu1-mesa_9.0.2-1_amd64.deb ...\n",
            "Unpacking libglu1-mesa:amd64 (9.0.2-1) ...\n",
            "Selecting previously unselected package libglu1-mesa-dev:amd64.\n",
            "Preparing to unpack .../10-libglu1-mesa-dev_9.0.2-1_amd64.deb ...\n",
            "Unpacking libglu1-mesa-dev:amd64 (9.0.2-1) ...\n",
            "Selecting previously unselected package libglew-dev:amd64.\n",
            "Preparing to unpack .../11-libglew-dev_2.2.0-4_amd64.deb ...\n",
            "Unpacking libglew-dev:amd64 (2.2.0-4) ...\n",
            "Selecting previously unselected package libglfw3:amd64.\n",
            "Preparing to unpack .../12-libglfw3_3.3.6-1_amd64.deb ...\n",
            "Unpacking libglfw3:amd64 (3.3.6-1) ...\n",
            "Selecting previously unselected package patchelf.\n",
            "Preparing to unpack .../13-patchelf_0.14.3-1_amd64.deb ...\n",
            "Unpacking patchelf (0.14.3-1) ...\n",
            "Selecting previously unselected package libosmesa6:amd64.\n",
            "Preparing to unpack .../14-libosmesa6_23.2.1-1ubuntu3.1~22.04.3_amd64.deb ...\n",
            "Unpacking libosmesa6:amd64 (23.2.1-1ubuntu3.1~22.04.3) ...\n",
            "Selecting previously unselected package libosmesa6-dev:amd64.\n",
            "Preparing to unpack .../15-libosmesa6-dev_23.2.1-1ubuntu3.1~22.04.3_amd64.deb ...\n",
            "Unpacking libosmesa6-dev:amd64 (23.2.1-1ubuntu3.1~22.04.3) ...\n",
            "Setting up libglfw3:amd64 (3.3.6-1) ...\n",
            "Setting up libglvnd-core-dev:amd64 (1.4.0-1) ...\n",
            "Setting up libgles1:amd64 (1.4.0-1) ...\n",
            "Setting up libglx-dev:amd64 (1.4.0-1) ...\n",
            "Setting up libglu1-mesa:amd64 (9.0.2-1) ...\n",
            "Setting up libopengl-dev:amd64 (1.4.0-1) ...\n",
            "Setting up patchelf (0.14.3-1) ...\n",
            "Setting up libgl-dev:amd64 (1.4.0-1) ...\n",
            "Setting up libosmesa6:amd64 (23.2.1-1ubuntu3.1~22.04.3) ...\n",
            "Setting up libegl-dev:amd64 (1.4.0-1) ...\n",
            "Setting up libglu1-mesa-dev:amd64 (9.0.2-1) ...\n",
            "Setting up libosmesa6-dev:amd64 (23.2.1-1ubuntu3.1~22.04.3) ...\n",
            "Setting up libgles-dev:amd64 (1.4.0-1) ...\n",
            "Setting up libglvnd-dev:amd64 (1.4.0-1) ...\n",
            "Setting up libglew-dev:amd64 (2.2.0-4) ...\n",
            "Setting up libgl1-mesa-dev:amd64 (23.2.1-1ubuntu3.1~22.04.3) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
            "\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Collecting pettingzoo\n",
            "  Downloading pettingzoo-1.25.0-py3-none-any.whl.metadata (8.9 kB)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from pettingzoo) (2.0.2)\n",
            "Requirement already satisfied: gymnasium>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from pettingzoo) (1.1.1)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium>=1.0.0->pettingzoo) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium>=1.0.0->pettingzoo) (4.13.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium>=1.0.0->pettingzoo) (0.0.4)\n",
            "Downloading pettingzoo-1.25.0-py3-none-any.whl (852 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m852.5/852.5 kB\u001b[0m \u001b[31m44.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pettingzoo\n",
            "Successfully installed pettingzoo-1.25.0\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: gymnasium in /usr/local/lib/python3.11/dist-packages (1.1.1)\n",
            "Collecting ray[rllib]\n",
            "  Downloading ray-2.46.0-cp311-cp311-manylinux2014_x86_64.whl.metadata (19 kB)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from ray[rllib]) (8.1.8)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from ray[rllib]) (3.18.0)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.11/dist-packages (from ray[rllib]) (4.23.0)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from ray[rllib]) (1.1.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from ray[rllib]) (24.2)\n",
            "Requirement already satisfied: protobuf!=3.19.5,>=3.15.3 in /usr/local/lib/python3.11/dist-packages (from ray[rllib]) (5.29.4)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from ray[rllib]) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from ray[rllib]) (2.32.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from ray[rllib]) (2.2.2)\n",
            "Collecting tensorboardX>=1.9 (from ray[rllib])\n",
            "  Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: pyarrow>=9.0.0 in /usr/local/lib/python3.11/dist-packages (from ray[rllib]) (18.1.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from ray[rllib]) (2025.3.2)\n",
            "Requirement already satisfied: dm_tree in /usr/local/lib/python3.11/dist-packages (from ray[rllib]) (0.1.9)\n",
            "Collecting gymnasium\n",
            "  Downloading gymnasium-1.0.0-py3-none-any.whl.metadata (9.5 kB)\n",
            "Collecting lz4 (from ray[rllib])\n",
            "  Downloading lz4-4.4.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
            "Collecting ormsgpack==1.7.0 (from ray[rllib])\n",
            "  Downloading ormsgpack-1.7.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.5/43.5 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from ray[rllib]) (1.15.2)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (2.0.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (4.13.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (0.0.4)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: absl-py>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from dm_tree->ray[rllib]) (1.4.0)\n",
            "Requirement already satisfied: attrs>=18.2.0 in /usr/local/lib/python3.11/dist-packages (from dm_tree->ray[rllib]) (25.3.0)\n",
            "Requirement already satisfied: wrapt>=1.11.2 in /usr/local/lib/python3.11/dist-packages (from dm_tree->ray[rllib]) (1.17.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema->ray[rllib]) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema->ray[rllib]) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema->ray[rllib]) (0.24.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->ray[rllib]) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->ray[rllib]) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->ray[rllib]) (2025.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->ray[rllib]) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->ray[rllib]) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->ray[rllib]) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->ray[rllib]) (2025.4.26)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->ray[rllib]) (1.17.0)\n",
            "Downloading gymnasium-1.0.0-py3-none-any.whl (958 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m958.1/958.1 kB\u001b[0m \u001b[31m61.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ormsgpack-1.7.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (220 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m220.6/220.6 kB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m122.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m92.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m61.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m44.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lz4-4.4.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m73.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ray-2.46.0-cp311-cp311-manylinux2014_x86_64.whl (68.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.5/68.5 MB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tensorboardX, ormsgpack, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, lz4, gymnasium, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, ray\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: gymnasium\n",
            "    Found existing installation: gymnasium 1.1.1\n",
            "    Uninstalling gymnasium-1.1.1:\n",
            "      Successfully uninstalled gymnasium-1.1.1\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed gymnasium-1.0.0 lz4-4.4.4 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 ormsgpack-1.7.0 ray-2.46.0 tensorboardX-2.6.2.2\n",
            "Collecting supersuit\n",
            "  Downloading supersuit-3.10.0-py3-none-any.whl.metadata (3.1 kB)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from supersuit) (2.0.2)\n",
            "Requirement already satisfied: gymnasium>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from supersuit) (1.0.0)\n",
            "Collecting tinyscaler>=1.2.6 (from supersuit)\n",
            "  Downloading tinyscaler-1.2.8-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium>=1.0.0->supersuit) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium>=1.0.0->supersuit) (4.13.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium>=1.0.0->supersuit) (0.0.4)\n",
            "Downloading supersuit-3.10.0-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.2/50.2 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tinyscaler-1.2.8-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (563 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m563.3/563.3 kB\u001b[0m \u001b[31m35.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tinyscaler, supersuit\n",
            "Successfully installed supersuit-3.10.0 tinyscaler-1.2.8\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "USING_COLAB = 'google.colab' in sys.modules\n",
        "\n",
        "if USING_COLAB:\n",
        "    !apt-get -qq update\n",
        "    !apt-get -qq install -y libosmesa6-dev libgl1-mesa-glx libglfw3 libgl1-mesa-dev libglew-dev patchelf\n",
        "    !apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n",
        "else:\n",
        "    !pip install torch torchvision torchaudio\n",
        "    !pip install numpy\n",
        "    !pip install tqdm\n",
        "    !pip install opencv-python\n",
        "\n",
        "!pip install matplotlib\n",
        "!pip install pettingzoo\n",
        "!pip install \"ray[rllib]\" torch gymnasium\n",
        "!pip install supersuit"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Environment Variables"
      ],
      "metadata": {
        "id": "5lLI59J2kbj0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"JUPYTER_PLATFORM_DIRS\"] = \"1\"\n",
        "os.environ[\"PYTHONWARNINGS\"] = \"ignore::DeprecationWarning\"\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
      ],
      "metadata": {
        "id": "-XTeqSfJkiPK"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Clone on-policy with rewards modifications\n",
        "Clone the repository into the personal drive, edit the requirements, and install it."
      ],
      "metadata": {
        "id": "-DyRS3ByVI8s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Update the git repository for on-policy\n",
        "# %cd on-policy\n",
        "# !git pull\n",
        "# %cd .."
      ],
      "metadata": {
        "id": "4FfrAB6v5kCz"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# If not cloned:\n",
        "if not os.path.isdir('on-policy'):\n",
        "    try:\n",
        "        print(f\"Getting Fresh copy\")\n",
        "        # Clone the repo\n",
        "        !git clone https://github.com/juji-lau/on-policy.git\n",
        "        sys.path.append('/content/drive/MyDrive/on-policy')\n",
        "\n",
        "        # Edit requirements.txt:\n",
        "        # !sed -i '/^absl-py==0\\.9\\.0$/d' on-policy/requirements.txt #(1, 1)\n",
        "        # !sed -i '/^atari-py==0\\.2\\.6$/d' on-policy/requirements.txt #(2, 7)\n",
        "        # # !sed -i 'contextvars' on-policy/requirements.txt\n",
        "        # # !sed -i 'enum34' on-policy/requirements.txt\n",
        "\n",
        "        # # Remove all version requirements (let pip figure it out)\n",
        "        # !sed -i '/^\\s*#/! s/[<>=!~].*$//' on-policy/requirements.txt\n",
        "\n",
        "        # Install the working requirements.txt\n",
        "        !pip install -r on-policy/requirements.txt\n",
        "        # Install on-policy\n",
        "        !pip install -e ./on-policy\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to clone, modify, and/or install the requirements.\")\n",
        "        print(f\"Removing any attempts at cloning.\")\n",
        "        !rm -r on-policy\n",
        "        print(f\"Got error message: {e}\")\n",
        "else:\n",
        "    print(f\"Using existing copy\")\n",
        "    # Just pip install the (modified) requirements.txt (they disappear every time)\n",
        "    sys.path.append(os.path.join(os.getcwd(), 'on-policy'))\n",
        "    # Install the working requirements.txt\n",
        "    !pip install -r on-policy/requirements.txt\n",
        "    # Install on-policy\n",
        "    !pip install -e ./on-policy"
      ],
      "metadata": {
        "collapsed": true,
        "id": "caUH5_JNPv_c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c49b855a-4371-4679-e74c-2efdeca88749"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using existing copy\n",
            "Collecting aioredis==1.3.1 (from -r on-policy/requirements.txt (line 1))\n",
            "  Downloading aioredis-1.3.1-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting astor==0.8.0 (from -r on-policy/requirements.txt (line 2))\n",
            "  Downloading astor-0.8.0-py2.py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting async-timeout==3.0.1 (from -r on-policy/requirements.txt (line 3))\n",
            "  Downloading async_timeout-3.0.1-py3-none-any.whl.metadata (4.0 kB)\n",
            "Collecting atari-py==0.2.6 (from -r on-policy/requirements.txt (line 4))\n",
            "  Downloading atari-py-0.2.6.tar.gz (790 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m790.2/790.2 kB\u001b[0m \u001b[31m44.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting atomicwrites==1.2.1 (from -r on-policy/requirements.txt (line 5))\n",
            "  Downloading atomicwrites-1.2.1-py2.py3-none-any.whl.metadata (5.5 kB)\n",
            "Collecting blessings==1.7 (from -r on-policy/requirements.txt (line 6))\n",
            "  Downloading blessings-1.7-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting colorama==0.4.3 (from -r on-policy/requirements.txt (line 7))\n",
            "  Downloading colorama-0.4.3-py2.py3-none-any.whl.metadata (14 kB)\n",
            "Collecting colorful==0.5.4 (from -r on-policy/requirements.txt (line 8))\n",
            "  Downloading colorful-0.5.4-py2.py3-none-any.whl.metadata (16 kB)\n",
            "Collecting configparser==5.0.1 (from -r on-policy/requirements.txt (line 9))\n",
            "  Downloading configparser-5.0.1-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting contextvars==2.4 (from -r on-policy/requirements.txt (line 10))\n",
            "  Downloading contextvars-2.4.tar.gz (9.6 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting deepdiff==4.3.2 (from -r on-policy/requirements.txt (line 11))\n",
            "  Downloading deepdiff-4.3.2-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting dill==0.3.2 (from -r on-policy/requirements.txt (line 12))\n",
            "  Downloading dill-0.3.2.zip (177 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.6/177.6 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting docopt==0.6.2 (from -r on-policy/requirements.txt (line 13))\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting fasteners==0.15 (from -r on-policy/requirements.txt (line 14))\n",
            "  Downloading fasteners-0.15-py2.py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting funcsigs==1.0.2 (from -r on-policy/requirements.txt (line 15))\n",
            "  Downloading funcsigs-1.0.2-py2.py3-none-any.whl.metadata (14 kB)\n",
            "Collecting gin==0.1.6 (from -r on-policy/requirements.txt (line 16))\n",
            "  Downloading gin-0.1.006.tar.bz2 (3.0 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting glfw==1.12.0 (from -r on-policy/requirements.txt (line 17))\n",
            "  Downloading glfw-1.12.0-py2.py27.py3.py30.py31.py32.py33.py34.py35.py36.py37.py38-none-manylinux2014_x86_64.whl.metadata (4.3 kB)\n",
            "Collecting gpustat==0.6.0 (from -r on-policy/requirements.txt (line 18))\n",
            "  Downloading gpustat-0.6.0.tar.gz (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.1/78.1 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting gql==0.2.0 (from -r on-policy/requirements.txt (line 19))\n",
            "  Downloading gql-0.2.0.tar.gz (18 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting graphql-core==1.1 (from -r on-policy/requirements.txt (line 20))\n",
            "  Downloading graphql-core-1.1.tar.gz (70 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.3/70.3 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting hiredis==1.1.0 (from -r on-policy/requirements.txt (line 21))\n",
            "  Downloading hiredis-1.1.0.tar.gz (54 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.6/54.6 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting idna-ssl==1.1.0 (from -r on-policy/requirements.txt (line 22))\n",
            "  Downloading idna-ssl-1.1.0.tar.gz (3.4 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting imageio==2.4.1 (from -r on-policy/requirements.txt (line 23))\n",
            "  Downloading imageio-2.4.1.tar.gz (3.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m93.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting immutables==0.14 (from -r on-policy/requirements.txt (line 24))\n",
            "  Downloading immutables-0.14.tar.gz (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.0/43.0 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting jsonnet==0.16.0 (from -r on-policy/requirements.txt (line 25))\n",
            "  Downloading jsonnet-0.16.0.tar.gz (256 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m257.0/257.0 kB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting Keras-Applications==1.0.8 (from -r on-policy/requirements.txt (line 26))\n",
            "  Downloading Keras_Applications-1.0.8-py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting Keras-Preprocessing==1.1.2 (from -r on-policy/requirements.txt (line 27))\n",
            "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting lockfile==0.12.2 (from -r on-policy/requirements.txt (line 28))\n",
            "  Downloading lockfile-0.12.2-py2.py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting mock==2.0.0 (from -r on-policy/requirements.txt (line 29))\n",
            "  Downloading mock-2.0.0-py2.py3-none-any.whl.metadata (3.2 kB)\n",
            "Collecting monotonic==1.5 (from -r on-policy/requirements.txt (line 30))\n",
            "  Downloading monotonic-1.5-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting mpyq==0.2.5 (from -r on-policy/requirements.txt (line 31))\n",
            "  Downloading mpyq-0.2.5.tar.gz (8.7 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting munch==2.3.2 (from -r on-policy/requirements.txt (line 32))\n",
            "  Downloading munch-2.3.2.tar.gz (7.6 kB)\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n",
            "Obtaining file:///content/drive/MyDrive/on-policy\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Installing collected packages: onpolicy\n",
            "  Running setup.py develop for onpolicy\n",
            "Successfully installed onpolicy-0.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Training and Visualizing MAPPO Environments with Different Reward Schemes**"
      ],
      "metadata": {
        "id": "xkhMunqctBCA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "YBXJZqSuWV0q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from pettingzoo.mpe import simple_spread_v2\n",
        "# change to the following later: from pettingzoo.mpe2 import simple_spread_v2\n",
        "import supersuit as ss"
      ],
      "metadata": {
        "id": "OinGTzOerkWj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70814a54-4b1e-440f-ce4d-a7c022a8ce11"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-f02c710eea48>:3: DeprecationWarning: The environment `pettingzoo.mpe` has been moved to `mpe2` and will be removed in a future release.Please update your imports.\n",
            "  from pettingzoo.mpe import simple_spread_v2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vynp2hcNO7mp"
      },
      "source": [
        "## Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ALGORITHM = \"mappo\"  #already mappo by default\n",
        "ENVIRONMENT = \"MPE\"\n",
        "USE_WANDB = False\n",
        "SCENARIO = \"simple_spread\"\n",
        "REWARD_TYPES = [\"individual\", \"partially_shared\", \"shared\", \"original\"]\n",
        "\n",
        "# Run specifiers\n",
        "NUM_ENV_STEPS = 100000\n",
        "EPISODE_LENGTH = 25\n",
        "NUM_TRAINING_THREADS = 1\n",
        "NUM_ROLLOUT_THREADS = 32\n",
        "N_EVAL_ROLLOUT_THREADS = 1\n",
        "SEED = 42\n",
        "\n",
        "# Simulation specifiers\n",
        "NUM_AGENTS = 3\n",
        "NUM_LANDMARKS = 3\n",
        "\n",
        "# Eval parameters\n",
        "USE_EVAL = True\n",
        "\n",
        "# Render parameters\n",
        "SAVE_GIFS = True\n",
        "USE_RENDER = True\n",
        "RENDER_EPISODES = 5\n"
      ],
      "metadata": {
        "id": "VdFzgcu0r0U2"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train MAPPO for Reward Types"
      ],
      "metadata": {
        "id": "Q_k5hMX1WhUu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train MAPPO using on-policy for a given reward type\n",
        "def train_mappo(reward_type):\n",
        "    assert (os.getcwd() == '/content/drive/MyDrive')\n",
        "\n",
        "    run_dir = f\"./runs/{SCENARIO}_{reward_type}\"\n",
        "    os.makedirs(run_dir, exist_ok=True)\n",
        "    print(f\"Train MAPPO: {reward_type}, Saving to: {run_dir}\")\n",
        "\n",
        "    # omitting --share_policy so that share_policy is set to True\n",
        "\n",
        "    !python on-policy/onpolicy/scripts/train/train_mpe.py \\\n",
        "        --algorithm_name $ALGORITHM \\\n",
        "        --experiment_name $reward_type \\\n",
        "        --seed $SEED \\\n",
        "        --num_env_steps $NUM_ENV_STEPS \\\n",
        "        --n_training_threads $NUM_TRAINING_THREADS \\\n",
        "        --n_rollout_threads $NUM_ROLLOUT_THREADS \\\n",
        "        --n_eval_rollout_threads $N_EVAL_ROLLOUT_THREADS \\\n",
        "        --use_wandb $USE_WANDB \\\n",
        "        --env_name $ENVIRONMENT \\\n",
        "        --episode_length $EPISODE_LENGTH \\\n",
        "        --use_eval $USE_EVAL \\\n",
        "        --save_gifs $SAVE_GIFS \\\n",
        "        --use_render $USE_RENDER \\\n",
        "        --render_episodes $RENDER_EPISODES \\\n",
        "        --scenario_name $SCENARIO \\\n",
        "        --num_landmarks $NUM_LANDMARKS \\\n",
        "        --num_agents $NUM_AGENTS \\\n",
        "        --reward_type $reward_type \\\n",
        "        --ppo_epoch 30 \\\n",
        "        --clip_param 0.1 \\\n",
        "        --entropy_coef 0.005 \\\n",
        "        --lr 5e-4 \\\n",
        "        --critic_lr 5e-4 \\\n",
        "        --use_valuenorm \\\n",
        "        --use_feature_normalization \\\n",
        "        --hidden_size 128 \\\n",
        "        --layer_N 2\n"
      ],
      "metadata": {
        "id": "vGhCwMptvVYg"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "DCgEYYoqf0hl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train and render for each reward type\n",
        "for reward in REWARD_TYPES:\n",
        "    print(f\"\\n--- Training with {reward} rewards ---\\n\", flush=True)\n",
        "    train_mappo(reward)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QOYurGZATnJP",
        "outputId": "91405b72-ff74-4dba-dbc4-a7d5d63e9a2a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Training with individual rewards ---\n",
            "\n",
            "Train MAPPO: individual, Saving to: ./runs/simple_spread_individual\n",
            "/content/drive/MyDrive/on-policy/onpolicy/envs/mpe/scenarios/__init__.py:1: DeprecationWarning: the imp module is deprecated in favour of importlib and slated for removal in Python 3.12; see the module's documentation for alternative uses\n",
            "  import imp\n",
            "u are choosing to use mappo, we set use_recurrent_policy & use_naive_recurrent_policy to be False\n",
            "choose to use gpu...\n",
            "Saved at /content/drive/MyDrive/runs/simple_spread_individual/run12.\n",
            "Namespace(algorithm_name='mappo', experiment_name='individual', seed=42, cuda=True, cuda_deterministic=True, n_training_threads=1, n_rollout_threads=32, n_eval_rollout_threads=1, n_render_rollout_threads=1, num_env_steps=100000, user_name='marl', use_wandb=False, env_name='MPE', use_obs_instead_of_state=False, episode_length=25, share_policy=True, use_centralized_V=True, stacked_frames=1, use_stacked_frames=False, hidden_size=128, layer_N=2, use_ReLU=True, use_popart=False, use_valuenorm=False, use_feature_normalization=False, use_orthogonal=True, gain=0.01, use_naive_recurrent_policy=False, use_recurrent_policy=False, recurrent_N=1, data_chunk_length=10, lr=0.0005, critic_lr=0.0005, opti_eps=1e-05, weight_decay=0, kl_threshold=0.01, ls_step=10, accept_ratio=0.5, ppo_epoch=30, use_clipped_value_loss=True, clip_param=0.1, num_mini_batch=1, entropy_coef=0.005, value_loss_coef=1, use_max_grad_norm=True, max_grad_norm=10.0, use_gae=True, gamma=0.99, gae_lambda=0.95, use_proper_time_limits=False, use_huber_loss=True, use_value_active_masks=True, use_policy_active_masks=True, huber_delta=10.0, use_linear_lr_decay=False, save_interval=1, log_interval=5, use_eval=True, eval_interval=25, eval_episodes=32, save_gifs=True, use_render=True, render_episodes=5, ifi=0.1, model_dir=None, encode_state=False, n_block=1, n_embd=64, n_head=1, dec_actor=False, share_actor=False, train_maps=None, eval_maps=None, run_dir='/content/drive/MyDrive/runs', scenario_name='simple_spread', num_landmarks=3, num_agents=3, reward_type='individual')\n",
            "/content/drive/MyDrive/runs/simple_spread_individual/run12\n",
            "obs_space:  [Box(-inf, inf, (18,), float32), Box(-inf, inf, (18,), float32), Box(-inf, inf, (18,), float32)]\n",
            "share_obs_space:  [Box(-inf, inf, (54,), float32), Box(-inf, inf, (54,), float32), Box(-inf, inf, (54,), float32)]\n",
            "act_space:  [Discrete(5), Discrete(5), Discrete(5)]\n",
            "\n",
            " Scenario simple_spread Algo mappo Exp individual updates 0/125 episodes, total num timesteps 800/100000, FPS 294.\n",
            "\n",
            "average episode rewards is -7.354166507720947\n",
            "policy entropy is 1.6058093150456747\n",
            "eval average episode rewards of agent: -10.333333333333334\n",
            "\n",
            " Scenario simple_spread Algo mappo Exp individual updates 5/125 episodes, total num timesteps 4800/100000, FPS 709.\n",
            "\n",
            "average episode rewards is -6.838541507720947\n",
            "policy entropy is 1.5684256116549173\n",
            "\n",
            " Scenario simple_spread Algo mappo Exp individual updates 10/125 episodes, total num timesteps 8800/100000, FPS 825.\n",
            "\n",
            "average episode rewards is -5.885416507720947\n",
            "policy entropy is 1.5315860112508137\n",
            "\n",
            " Scenario simple_spread Algo mappo Exp individual updates 15/125 episodes, total num timesteps 12800/100000, FPS 811.\n",
            "\n",
            "average episode rewards is -5.677083492279053\n",
            "policy entropy is 1.4904208223025004\n",
            "\n",
            " Scenario simple_spread Algo mappo Exp individual updates 20/125 episodes, total num timesteps 16800/100000, FPS 838.\n",
            "\n",
            "average episode rewards is -5.078125\n",
            "policy entropy is 1.4518693566322327\n",
            "\n",
            " Scenario simple_spread Algo mappo Exp individual updates 25/125 episodes, total num timesteps 20800/100000, FPS 867.\n",
            "\n",
            "average episode rewards is -5.578125\n",
            "policy entropy is 1.410754426320394\n",
            "eval average episode rewards of agent: -3.5\n",
            "\n",
            " Scenario simple_spread Algo mappo Exp individual updates 30/125 episodes, total num timesteps 24800/100000, FPS 878.\n",
            "\n",
            "average episode rewards is -6.541666507720947\n",
            "policy entropy is 1.3678233663241068\n",
            "\n",
            " Scenario simple_spread Algo mappo Exp individual updates 35/125 episodes, total num timesteps 28800/100000, FPS 871.\n",
            "\n",
            "average episode rewards is -5.921875\n",
            "policy entropy is 1.3018044114112854\n",
            "\n",
            " Scenario simple_spread Algo mappo Exp individual updates 40/125 episodes, total num timesteps 32800/100000, FPS 888.\n",
            "\n",
            "average episode rewards is -4.71875\n",
            "policy entropy is 1.2601624011993409\n",
            "\n",
            " Scenario simple_spread Algo mappo Exp individual updates 45/125 episodes, total num timesteps 36800/100000, FPS 903.\n",
            "\n",
            "average episode rewards is -5.135416507720947\n",
            "policy entropy is 1.2446071227391562\n",
            "\n",
            " Scenario simple_spread Algo mappo Exp individual updates 50/125 episodes, total num timesteps 40800/100000, FPS 887.\n",
            "\n",
            "average episode rewards is -5.411458492279053\n",
            "policy entropy is 1.1294858614603678\n",
            "eval average episode rewards of agent: -5.833333333333333\n",
            "\n",
            " Scenario simple_spread Algo mappo Exp individual updates 55/125 episodes, total num timesteps 44800/100000, FPS 896.\n",
            "\n",
            "average episode rewards is -5.53125\n",
            "policy entropy is 1.1262864470481873\n",
            "\n",
            " Scenario simple_spread Algo mappo Exp individual updates 60/125 episodes, total num timesteps 48800/100000, FPS 907.\n",
            "\n",
            "average episode rewards is -3.5364582538604736\n",
            "policy entropy is 1.0550543467203777\n",
            "\n",
            " Scenario simple_spread Algo mappo Exp individual updates 65/125 episodes, total num timesteps 52800/100000, FPS 900.\n",
            "\n",
            "average episode rewards is -5.5625\n",
            "policy entropy is 1.0426297346750895\n",
            "\n",
            " Scenario simple_spread Algo mappo Exp individual updates 70/125 episodes, total num timesteps 56800/100000, FPS 904.\n",
            "\n",
            "average episode rewards is -6.328125476837158\n",
            "policy entropy is 1.029319409529368\n",
            "\n",
            " Scenario simple_spread Algo mappo Exp individual updates 75/125 episodes, total num timesteps 60800/100000, FPS 913.\n",
            "\n",
            "average episode rewards is -4.364583492279053\n",
            "policy entropy is 0.9914040088653564\n",
            "eval average episode rewards of agent: -10.833333333333334\n",
            "\n",
            " Scenario simple_spread Algo mappo Exp individual updates 80/125 episodes, total num timesteps 64800/100000, FPS 917.\n",
            "\n",
            "average episode rewards is -7.072916507720947\n",
            "policy entropy is 0.9330834408601125\n",
            "\n",
            " Scenario simple_spread Algo mappo Exp individual updates 85/125 episodes, total num timesteps 68800/100000, FPS 909.\n",
            "\n",
            "average episode rewards is -4.859375\n",
            "policy entropy is 0.8940029362837474\n",
            "\n",
            " Scenario simple_spread Algo mappo Exp individual updates 90/125 episodes, total num timesteps 72800/100000, FPS 914.\n",
            "\n",
            "average episode rewards is -4.364583492279053\n",
            "policy entropy is 0.9189858833948771\n",
            "\n",
            " Scenario simple_spread Algo mappo Exp individual updates 95/125 episodes, total num timesteps 76800/100000, FPS 918.\n",
            "\n",
            "average episode rewards is -3.718750238418579\n",
            "policy entropy is 0.901981912056605\n",
            "\n",
            " Scenario simple_spread Algo mappo Exp individual updates 100/125 episodes, total num timesteps 80800/100000, FPS 910.\n",
            "\n",
            "average episode rewards is -5.7760419845581055\n",
            "policy entropy is 0.8746160050233205\n",
            "eval average episode rewards of agent: 8.5\n",
            "\n",
            " Scenario simple_spread Algo mappo Exp individual updates 105/125 episodes, total num timesteps 84800/100000, FPS 915.\n",
            "\n",
            "average episode rewards is -4.854166507720947\n",
            "policy entropy is 0.8610180974006653\n",
            "\n",
            " Scenario simple_spread Algo mappo Exp individual updates 110/125 episodes, total num timesteps 88800/100000, FPS 920.\n",
            "\n",
            "average episode rewards is -3.6510415077209473\n",
            "policy entropy is 0.8820757428805034\n",
            "\n",
            " Scenario simple_spread Algo mappo Exp individual updates 115/125 episodes, total num timesteps 92800/100000, FPS 919.\n",
            "\n",
            "average episode rewards is -4.244791507720947\n",
            "policy entropy is 0.8577275554339091\n",
            "\n",
            " Scenario simple_spread Algo mappo Exp individual updates 120/125 episodes, total num timesteps 96800/100000, FPS 917.\n",
            "\n",
            "average episode rewards is -3.859375\n",
            "policy entropy is 0.8098569174607595\n",
            "\n",
            "--- Training with partially_shared rewards ---\n",
            "\n",
            "Train MAPPO: partially_shared, Saving to: ./runs/simple_spread_partially_shared\n",
            "/content/drive/MyDrive/on-policy/onpolicy/envs/mpe/scenarios/__init__.py:1: DeprecationWarning: the imp module is deprecated in favour of importlib and slated for removal in Python 3.12; see the module's documentation for alternative uses\n",
            "  import imp\n",
            "u are choosing to use mappo, we set use_recurrent_policy & use_naive_recurrent_policy to be False\n",
            "choose to use gpu...\n",
            "Saved at /content/drive/MyDrive/runs/simple_spread_partially_shared/run12.\n",
            "Namespace(algorithm_name='mappo', experiment_name='partially_shared', seed=42, cuda=True, cuda_deterministic=True, n_training_threads=1, n_rollout_threads=32, n_eval_rollout_threads=1, n_render_rollout_threads=1, num_env_steps=100000, user_name='marl', use_wandb=False, env_name='MPE', use_obs_instead_of_state=False, episode_length=25, share_policy=True, use_centralized_V=True, stacked_frames=1, use_stacked_frames=False, hidden_size=128, layer_N=2, use_ReLU=True, use_popart=False, use_valuenorm=False, use_feature_normalization=False, use_orthogonal=True, gain=0.01, use_naive_recurrent_policy=False, use_recurrent_policy=False, recurrent_N=1, data_chunk_length=10, lr=0.0005, critic_lr=0.0005, opti_eps=1e-05, weight_decay=0, kl_threshold=0.01, ls_step=10, accept_ratio=0.5, ppo_epoch=30, use_clipped_value_loss=True, clip_param=0.1, num_mini_batch=1, entropy_coef=0.005, value_loss_coef=1, use_max_grad_norm=True, max_grad_norm=10.0, use_gae=True, gamma=0.99, gae_lambda=0.95, use_proper_time_limits=False, use_huber_loss=True, use_value_active_masks=True, use_policy_active_masks=True, huber_delta=10.0, use_linear_lr_decay=False, save_interval=1, log_interval=5, use_eval=True, eval_interval=25, eval_episodes=32, save_gifs=True, use_render=True, render_episodes=5, ifi=0.1, model_dir=None, encode_state=False, n_block=1, n_embd=64, n_head=1, dec_actor=False, share_actor=False, train_maps=None, eval_maps=None, run_dir='/content/drive/MyDrive/runs', scenario_name='simple_spread', num_landmarks=3, num_agents=3, reward_type='partially_shared')\n",
            "/content/drive/MyDrive/runs/simple_spread_partially_shared/run12\n",
            "obs_space:  [Box(-inf, inf, (18,), float32), Box(-inf, inf, (18,), float32), Box(-inf, inf, (18,), float32)]\n",
            "share_obs_space:  [Box(-inf, inf, (54,), float32), Box(-inf, inf, (54,), float32), Box(-inf, inf, (54,), float32)]\n",
            "act_space:  [Discrete(5), Discrete(5), Discrete(5)]\n",
            "\n",
            " Scenario simple_spread Algo mappo Exp partially_shared updates 0/125 episodes, total num timesteps 800/100000, FPS 404.\n",
            "\n",
            "average episode rewards is -9.417508125305176\n",
            "policy entropy is 1.6061816573143006\n",
            "eval average episode rewards of agent: -12.619528899850257\n",
            "\n",
            " Scenario simple_spread Algo mappo Exp partially_shared updates 5/125 episodes, total num timesteps 4800/100000, FPS 419.\n",
            "\n",
            "average episode rewards is -8.114076614379883\n",
            "policy entropy is 1.576020848751068\n",
            "\n",
            " Scenario simple_spread Algo mappo Exp partially_shared updates 10/125 episodes, total num timesteps 8800/100000, FPS 511.\n",
            "\n",
            "average episode rewards is -6.839717864990234\n",
            "policy entropy is 1.5470507502555848\n",
            "\n",
            " Scenario simple_spread Algo mappo Exp partially_shared updates 15/125 episodes, total num timesteps 12800/100000, FPS 503.\n",
            "\n",
            "average episode rewards is -7.000908851623535\n",
            "policy entropy is 1.51136714220047\n",
            "\n",
            " Scenario simple_spread Algo mappo Exp partially_shared updates 20/125 episodes, total num timesteps 16800/100000, FPS 518.\n",
            "\n",
            "average episode rewards is -6.983243465423584\n",
            "policy entropy is 1.4730740229288737\n",
            "\n",
            " Scenario simple_spread Algo mappo Exp partially_shared updates 25/125 episodes, total num timesteps 20800/100000, FPS 508.\n",
            "\n",
            "average episode rewards is -8.397533416748047\n",
            "policy entropy is 1.4314993937810263\n",
            "eval average episode rewards of agent: 1.3760322818962116\n",
            "\n",
            " Scenario simple_spread Algo mappo Exp partially_shared updates 30/125 episodes, total num timesteps 24800/100000, FPS 527.\n",
            "\n",
            "average episode rewards is -7.784792900085449\n",
            "policy entropy is 1.366504927476247\n",
            "\n",
            " Scenario simple_spread Algo mappo Exp partially_shared updates 35/125 episodes, total num timesteps 28800/100000, FPS 524.\n",
            "\n",
            "average episode rewards is -6.675398349761963\n",
            "policy entropy is 1.2959771196047465\n",
            "\n",
            " Scenario simple_spread Algo mappo Exp partially_shared updates 40/125 episodes, total num timesteps 32800/100000, FPS 538.\n",
            "\n",
            "average episode rewards is -4.8035502433776855\n",
            "policy entropy is 1.1995249470074971\n",
            "\n",
            " Scenario simple_spread Algo mappo Exp partially_shared updates 45/125 episodes, total num timesteps 36800/100000, FPS 534.\n",
            "\n",
            "average episode rewards is -5.022518157958984\n",
            "policy entropy is 1.1149992227554322\n",
            "\n",
            " Scenario simple_spread Algo mappo Exp partially_shared updates 50/125 episodes, total num timesteps 40800/100000, FPS 545.\n",
            "\n",
            "average episode rewards is -4.791497707366943\n",
            "policy entropy is 1.033614695072174\n",
            "eval average episode rewards of agent: -1.648263065651985\n",
            "\n",
            " Scenario simple_spread Algo mappo Exp partially_shared updates 55/125 episodes, total num timesteps 44800/100000, FPS 542.\n",
            "\n",
            "average episode rewards is -4.1169753074646\n",
            "policy entropy is 0.949631275733312\n",
            "\n",
            " Scenario simple_spread Algo mappo Exp partially_shared updates 60/125 episodes, total num timesteps 48800/100000, FPS 550.\n",
            "\n",
            "average episode rewards is -5.5228376388549805\n",
            "policy entropy is 0.8707527180512746\n",
            "\n",
            " Scenario simple_spread Algo mappo Exp partially_shared updates 65/125 episodes, total num timesteps 52800/100000, FPS 548.\n",
            "\n",
            "average episode rewards is -4.579498767852783\n",
            "policy entropy is 0.7931851426760356\n",
            "\n",
            " Scenario simple_spread Algo mappo Exp partially_shared updates 70/125 episodes, total num timesteps 56800/100000, FPS 555.\n",
            "\n",
            "average episode rewards is -4.901242733001709\n",
            "policy entropy is 0.7612326780954997\n",
            "\n",
            " Scenario simple_spread Algo mappo Exp partially_shared updates 75/125 episodes, total num timesteps 60800/100000, FPS 559.\n",
            "\n",
            "average episode rewards is -3.9260871410369873\n",
            "policy entropy is 0.6445211072762808\n",
            "eval average episode rewards of agent: -1.930899922482593\n",
            "\n",
            " Scenario simple_spread Algo mappo Exp partially_shared updates 80/125 episodes, total num timesteps 64800/100000, FPS 561.\n",
            "\n",
            "average episode rewards is -4.315652847290039\n",
            "policy entropy is 0.6813297986984252\n",
            "\n",
            " Scenario simple_spread Algo mappo Exp partially_shared updates 85/125 episodes, total num timesteps 68800/100000, FPS 567.\n",
            "\n",
            "average episode rewards is -3.8956496715545654\n",
            "policy entropy is 0.5901824275652567\n",
            "\n",
            " Scenario simple_spread Algo mappo Exp partially_shared updates 90/125 episodes, total num timesteps 72800/100000, FPS 565.\n",
            "\n",
            "average episode rewards is -5.264400482177734\n",
            "policy entropy is 0.7030413150787354\n",
            "\n",
            " Scenario simple_spread Algo mappo Exp partially_shared updates 95/125 episodes, total num timesteps 76800/100000, FPS 571.\n",
            "\n",
            "average episode rewards is -5.536960601806641\n",
            "policy entropy is 0.6065222163995106\n",
            "\n",
            " Scenario simple_spread Algo mappo Exp partially_shared updates 100/125 episodes, total num timesteps 80800/100000, FPS 569.\n",
            "\n",
            "average episode rewards is -4.223999977111816\n",
            "policy entropy is 0.48196497857570647\n",
            "eval average episode rewards of agent: -2.0163692017975796\n",
            "\n",
            " Scenario simple_spread Algo mappo Exp partially_shared updates 105/125 episodes, total num timesteps 84800/100000, FPS 575.\n",
            "\n",
            "average episode rewards is -4.7032246589660645\n",
            "policy entropy is 0.5169966518878937\n",
            "\n",
            " Scenario simple_spread Algo mappo Exp partially_shared updates 110/125 episodes, total num timesteps 88800/100000, FPS 573.\n",
            "\n",
            "average episode rewards is -3.6338183879852295\n",
            "policy entropy is 0.5151734371980031\n",
            "\n",
            " Scenario simple_spread Algo mappo Exp partially_shared updates 115/125 episodes, total num timesteps 92800/100000, FPS 576.\n",
            "\n",
            "average episode rewards is -3.4900214672088623\n",
            "policy entropy is 0.47300992111365\n",
            "\n",
            " Scenario simple_spread Algo mappo Exp partially_shared updates 120/125 episodes, total num timesteps 96800/100000, FPS 579.\n",
            "\n",
            "average episode rewards is -5.337104797363281\n",
            "policy entropy is 0.5718981782595317\n",
            "\n",
            "--- Training with shared rewards ---\n",
            "\n",
            "Train MAPPO: shared, Saving to: ./runs/simple_spread_shared\n",
            "/content/drive/MyDrive/on-policy/onpolicy/envs/mpe/scenarios/__init__.py:1: DeprecationWarning: the imp module is deprecated in favour of importlib and slated for removal in Python 3.12; see the module's documentation for alternative uses\n",
            "  import imp\n",
            "u are choosing to use mappo, we set use_recurrent_policy & use_naive_recurrent_policy to be False\n",
            "choose to use gpu...\n",
            "Saved at /content/drive/MyDrive/runs/simple_spread_shared/run12.\n",
            "Namespace(algorithm_name='mappo', experiment_name='shared', seed=42, cuda=True, cuda_deterministic=True, n_training_threads=1, n_rollout_threads=32, n_eval_rollout_threads=1, n_render_rollout_threads=1, num_env_steps=100000, user_name='marl', use_wandb=False, env_name='MPE', use_obs_instead_of_state=False, episode_length=25, share_policy=True, use_centralized_V=True, stacked_frames=1, use_stacked_frames=False, hidden_size=128, layer_N=2, use_ReLU=True, use_popart=False, use_valuenorm=False, use_feature_normalization=False, use_orthogonal=True, gain=0.01, use_naive_recurrent_policy=False, use_recurrent_policy=False, recurrent_N=1, data_chunk_length=10, lr=0.0005, critic_lr=0.0005, opti_eps=1e-05, weight_decay=0, kl_threshold=0.01, ls_step=10, accept_ratio=0.5, ppo_epoch=30, use_clipped_value_loss=True, clip_param=0.1, num_mini_batch=1, entropy_coef=0.005, value_loss_coef=1, use_max_grad_norm=True, max_grad_norm=10.0, use_gae=True, gamma=0.99, gae_lambda=0.95, use_proper_time_limits=False, use_huber_loss=True, use_value_active_masks=True, use_policy_active_masks=True, huber_delta=10.0, use_linear_lr_decay=False, save_interval=1, log_interval=5, use_eval=True, eval_interval=25, eval_episodes=32, save_gifs=True, use_render=True, render_episodes=5, ifi=0.1, model_dir=None, encode_state=False, n_block=1, n_embd=64, n_head=1, dec_actor=False, share_actor=False, train_maps=None, eval_maps=None, run_dir='/content/drive/MyDrive/runs', scenario_name='simple_spread', num_landmarks=3, num_agents=3, reward_type='shared')\n",
            "/content/drive/MyDrive/runs/simple_spread_shared/run12\n",
            "obs_space:  [Box(-inf, inf, (18,), float32), Box(-inf, inf, (18,), float32), Box(-inf, inf, (18,), float32)]\n",
            "share_obs_space:  [Box(-inf, inf, (54,), float32), Box(-inf, inf, (54,), float32), Box(-inf, inf, (54,), float32)]\n",
            "act_space:  [Discrete(5), Discrete(5), Discrete(5)]\n",
            "\n",
            " Scenario simple_spread Algo mappo Exp shared updates 0/125 episodes, total num timesteps 800/100000, FPS 550.\n",
            "\n",
            "average episode rewards is -1.7374236583709717\n",
            "policy entropy is 1.6062516450881958\n",
            "eval average episode rewards of agent: -6.271565077538803\n",
            "\n",
            " Scenario simple_spread Algo mappo Exp shared updates 5/125 episodes, total num timesteps 4800/100000, FPS 750.\n",
            "\n",
            "average episode rewards is -1.9621318578720093\n",
            "policy entropy is 1.5840506315231324\n",
            "\n",
            " Scenario simple_spread Algo mappo Exp shared updates 10/125 episodes, total num timesteps 8800/100000, FPS 684.\n",
            "\n",
            "average episode rewards is -1.2975324392318726\n",
            "policy entropy is 1.5634700576464335\n",
            "\n",
            " Scenario simple_spread Algo mappo Exp shared updates 15/125 episodes, total num timesteps 12800/100000, FPS 720.\n",
            "\n",
            "average episode rewards is -1.5293726921081543\n",
            "policy entropy is 1.5297187447547913\n",
            "\n",
            " Scenario simple_spread Algo mappo Exp shared updates 20/125 episodes, total num timesteps 16800/100000, FPS 739.\n",
            "\n",
            "average episode rewards is -1.7355207204818726\n",
            "policy entropy is 1.5034501989682516\n",
            "\n",
            " Scenario simple_spread Algo mappo Exp shared updates 25/125 episodes, total num timesteps 20800/100000, FPS 719.\n",
            "\n",
            "average episode rewards is -1.3548922538757324\n",
            "policy entropy is 1.4783956209818523\n",
            "eval average episode rewards of agent: 2.090723052352438\n",
            "\n",
            " Scenario simple_spread Algo mappo Exp shared updates 30/125 episodes, total num timesteps 24800/100000, FPS 733.\n",
            "\n",
            "average episode rewards is -0.8094191551208496\n",
            "policy entropy is 1.45495396455129\n",
            "\n",
            " Scenario simple_spread Algo mappo Exp shared updates 35/125 episodes, total num timesteps 28800/100000, FPS 722.\n",
            "\n",
            "average episode rewards is -1.629225730895996\n",
            "policy entropy is 1.4267144997914631\n",
            "\n",
            " Scenario simple_spread Algo mappo Exp shared updates 40/125 episodes, total num timesteps 32800/100000, FPS 725.\n",
            "\n",
            "average episode rewards is -0.870822012424469\n",
            "policy entropy is 1.3649980942408244\n",
            "\n",
            " Scenario simple_spread Algo mappo Exp shared updates 45/125 episodes, total num timesteps 36800/100000, FPS 734.\n",
            "\n",
            "average episode rewards is -1.760197639465332\n",
            "policy entropy is 1.3123366554578146\n",
            "\n",
            " Scenario simple_spread Algo mappo Exp shared updates 50/125 episodes, total num timesteps 40800/100000, FPS 723.\n",
            "\n",
            "average episode rewards is -2.220735549926758\n",
            "policy entropy is 1.278650951385498\n",
            "eval average episode rewards of agent: -1.0096653111368816\n",
            "\n",
            " Scenario simple_spread Algo mappo Exp shared updates 55/125 episodes, total num timesteps 44800/100000, FPS 731.\n",
            "\n",
            "average episode rewards is -1.7279835939407349\n",
            "policy entropy is 1.1967042048772176\n",
            "\n",
            " Scenario simple_spread Algo mappo Exp shared updates 60/125 episodes, total num timesteps 48800/100000, FPS 736.\n",
            "\n",
            "average episode rewards is -1.8045568466186523\n",
            "policy entropy is 1.1168212056159974\n",
            "\n",
            " Scenario simple_spread Algo mappo Exp shared updates 65/125 episodes, total num timesteps 52800/100000, FPS 728.\n",
            "\n",
            "average episode rewards is -1.2624526023864746\n",
            "policy entropy is 1.0471224943796793\n",
            "\n",
            " Scenario simple_spread Algo mappo Exp shared updates 70/125 episodes, total num timesteps 56800/100000, FPS 735.\n",
            "\n",
            "average episode rewards is -1.2191134691238403\n",
            "policy entropy is 0.9829276303450266\n",
            "\n",
            " Scenario simple_spread Algo mappo Exp shared updates 75/125 episodes, total num timesteps 60800/100000, FPS 727.\n",
            "\n",
            "average episode rewards is -1.4853153228759766\n",
            "policy entropy is 0.9037918408711751\n",
            "eval average episode rewards of agent: 0.6569461909181351\n",
            "\n",
            " Scenario simple_spread Algo mappo Exp shared updates 80/125 episodes, total num timesteps 64800/100000, FPS 729.\n",
            "\n",
            "average episode rewards is -1.5674563646316528\n",
            "policy entropy is 0.8331575334072113\n",
            "\n",
            " Scenario simple_spread Algo mappo Exp shared updates 85/125 episodes, total num timesteps 68800/100000, FPS 734.\n",
            "\n",
            "average episode rewards is -1.2952961921691895\n",
            "policy entropy is 0.7816055516401926\n",
            "\n",
            " Scenario simple_spread Algo mappo Exp shared updates 90/125 episodes, total num timesteps 72800/100000, FPS 727.\n",
            "\n",
            "average episode rewards is -0.5269766449928284\n",
            "policy entropy is 0.7486170689264934\n",
            "\n",
            " Scenario simple_spread Algo mappo Exp shared updates 95/125 episodes, total num timesteps 76800/100000, FPS 730.\n",
            "\n",
            "average episode rewards is -1.5582612752914429\n",
            "policy entropy is 0.7082979222138722\n",
            "\n",
            " Scenario simple_spread Algo mappo Exp shared updates 100/125 episodes, total num timesteps 80800/100000, FPS 729.\n",
            "\n",
            "average episode rewards is -0.9636729955673218\n",
            "policy entropy is 0.6112930655479432\n",
            "eval average episode rewards of agent: -0.15643739166957923\n",
            "\n",
            " Scenario simple_spread Algo mappo Exp shared updates 105/125 episodes, total num timesteps 84800/100000, FPS 726.\n",
            "\n",
            "average episode rewards is -1.1130980253219604\n",
            "policy entropy is 0.6198614438374838\n",
            "\n",
            " Scenario simple_spread Algo mappo Exp shared updates 110/125 episodes, total num timesteps 88800/100000, FPS 730.\n",
            "\n",
            "average episode rewards is -0.8859505653381348\n",
            "policy entropy is 0.5582842747370402\n",
            "\n",
            " Scenario simple_spread Algo mappo Exp shared updates 115/125 episodes, total num timesteps 92800/100000, FPS 724.\n",
            "\n",
            "average episode rewards is -1.3883541822433472\n",
            "policy entropy is 0.5708299775918325\n",
            "\n",
            " Scenario simple_spread Algo mappo Exp shared updates 120/125 episodes, total num timesteps 96800/100000, FPS 727.\n",
            "\n",
            "average episode rewards is -0.7791668772697449\n",
            "policy entropy is 0.5311177174250284\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Logging the Metrics\n",
        "Copy the output from train_mappo for each reward type and put it in on-policy/logging/{reward_type}.txt"
      ],
      "metadata": {
        "id": "K01G5r3lnxTB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for reward in REWARD_TYPES:\n",
        "    text_file = f\"on-policy/logging/{reward}.txt\"\n",
        "    !python on-policy/logging/log_metrics.py --log_file $text_file --reward_type $reward"
      ],
      "metadata": {
        "id": "s2xLrsFUXkDu"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Render MAPPO for all Reward Types"
      ],
      "metadata": {
        "id": "uTZMG33XjQ5q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Render MAPPO using on-policy for a given reward type\n",
        "def render_mappo_5(reward_type, run_number):\n",
        "    assert (os.getcwd() == '/content/drive/MyDrive')\n",
        "\n",
        "    model_dir = f\"runs/{SCENARIO}_{reward_type}\"\n",
        "\n",
        "    # omitting --share_policy so that share_policy is set to True\n",
        "    !python on-policy/onpolicy/scripts/render/render_mpe.py \\\n",
        "        --algorithm_name $ALGORITHM \\\n",
        "        --experiment_name $reward_type \\\n",
        "        --seed $SEED \\\n",
        "        --num_env_steps $NUM_ENV_STEPS \\\n",
        "        --n_training_threads $NUM_TRAINING_THREADS \\\n",
        "        --n_rollout_threads 1 \\\n",
        "        --n_eval_rollout_threads $N_EVAL_ROLLOUT_THREADS \\\n",
        "        --use_wandb $USE_WANDB \\\n",
        "        --env_name $ENVIRONMENT \\\n",
        "        --episode_length $EPISODE_LENGTH \\\n",
        "        --use_eval $USE_EVAL \\\n",
        "        --save_gifs $SAVE_GIFS \\\n",
        "        --use_render $USE_RENDER \\\n",
        "        --render_episodes $RENDER_EPISODES \\\n",
        "        --model_dir $model_dir \\\n",
        "        --scenario_name $SCENARIO \\\n",
        "        --num_landmarks $NUM_LANDMARKS \\\n",
        "        --num_agents $NUM_AGENTS \\\n",
        "        --reward_type $reward_type \\\n",
        "        --run_number $run_number \\\n",
        "        --use_valuenorm \\\n",
        "        --use_feature_normalization \\\n",
        "        --hidden_size 128 \\\n",
        "        --layer_N 2"
      ],
      "metadata": {
        "id": "Mjc3JCmnTuCL"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install -y xvfb\n",
        "!pip install pyvirtualdisplay\n",
        "!pip install pyglet==1.4.10 # need to downgrade pyglet to 1.4.10 otherwise this won't work\n",
        "\n",
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=False, size=(1400, 900))\n",
        "display.start()\n",
        "\n",
        "run_num = -1\n",
        "try:\n",
        "    for reward in REWARD_TYPES:\n",
        "        render_mappo_5(reward, run_num)\n",
        "    display.stop()\n",
        "except Exception as e:\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "    display.stop()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jTQw3X3OtViB",
        "outputId": "40d8a434-bb3e-4c18-aa90-90b272b06152"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "xvfb is already the newest version (2:21.1.4-2ubuntu1.7~22.04.14).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 35 not upgraded.\n",
            "Requirement already satisfied: pyvirtualdisplay in /usr/local/lib/python3.11/dist-packages (3.0)\n",
            "Requirement already satisfied: pyglet==1.4.10 in /usr/local/lib/python3.11/dist-packages (1.4.10)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.11/dist-packages (from pyglet==1.4.10) (1.0.0)\n",
            "/content/drive/MyDrive/on-policy/onpolicy/envs/mpe/scenarios/__init__.py:1: DeprecationWarning: the imp module is deprecated in favour of importlib and slated for removal in Python 3.12; see the module's documentation for alternative uses\n",
            "  import imp\n",
            "u are choosing to use mappo, we set use_recurrent_policy & use_naive_recurrent_policy to be False\n",
            "choose to use gpu...\n",
            "DEBUGGING ALL ARGS: runs/simple_spread_original/run1/models \n",
            " (render_mpe.py)\n",
            "Run dir: /content/drive/MyDrive/runs/simple_spread_original/run1\n",
            "/content/drive/MyDrive/runs/simple_spread_original/run1\n",
            "obs_space:  [Box(-inf, inf, (18,), float32), Box(-inf, inf, (18,), float32), Box(-inf, inf, (18,), float32)]\n",
            "share_obs_space:  [Box(-inf, inf, (54,), float32), Box(-inf, inf, (54,), float32), Box(-inf, inf, (54,), float32)]\n",
            "act_space:  [Discrete(5), Discrete(5), Discrete(5)]\n",
            "/usr/local/lib/python3.11/dist-packages/gym/core.py:43: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  deprecation(\n",
            "/content/drive/MyDrive/on-policy/onpolicy/envs/mpe/rendering.py:115: DeprecationWarning: The binary mode of fromstring is deprecated, as it behaves surprisingly on unicode inputs. Use frombuffer instead\n",
            "  arr = np.fromstring(image_data.get_data(), dtype=np.uint8, sep='')\n",
            "/usr/local/lib/python3.11/dist-packages/gym/core.py:43: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  deprecation(\n",
            "average episode rewards is: -60.2214656602604\n",
            "average episode rewards is: -69.59391479175864\n",
            "average episode rewards is: -77.0146454241317\n",
            "average episode rewards is: -114.19169255337435\n",
            "average episode rewards is: -66.32474513882332\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Git Commands"
      ],
      "metadata": {
        "id": "kPF6RgmmpOTE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def set_remote():\n",
        "    import os\n",
        "    import getpass\n",
        "\n",
        "    # 🔒 Ask for token securely\n",
        "    token = getpass.getpass('Enter your GitHub PAT: ')\n",
        "\n",
        "    # ✅ Set your git identity\n",
        "    !git config --global user.email \"\"\n",
        "    !git config --global user.name \"\"\n",
        "\n",
        "    # ✅ Set the new remote using token auth\n",
        "    remote_url = f\"https://{token}@github.com/juji-lau/on-policy.git\"\n",
        "    !git remote set-url origin {remote_url}\n",
        "\n",
        "# set_remote()\n",
        "# # ✅ Add, commit, and push\n",
        "# !git add .\n",
        "# !git commit -m \"Final changes to on-policy fork.\"\n",
        "# !git push origin main"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M4sVpKV7797d",
        "outputId": "790033c1-86a8-4099-a83d-5567deace56e"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[main c0d345e] Final changes to on-policy fork.\n",
            " 33 files changed, 727 insertions(+), 65 deletions(-)\n",
            " create mode 100644 logging/individual.png\n",
            " create mode 100644 logging/individual.txt\n",
            " create mode 100644 logging/log_metrics.py\n",
            " create mode 100644 logging/original.png\n",
            " create mode 100644 logging/original.txt\n",
            " create mode 100644 logging/partially_shared.png\n",
            " create mode 100644 logging/partially_shared.txt\n",
            " create mode 100644 logging/shared.png\n",
            " create mode 100644 logging/shared.txt\n",
            " mode change 100755 => 100644 onpolicy/scripts/train_mpe_scripts/train_mpe_comm.sh\n",
            " mode change 100755 => 100644 onpolicy/scripts/train_mpe_scripts/train_mpe_reference.sh\n",
            " mode change 100755 => 100644 onpolicy/scripts/train_mpe_scripts/train_mpe_spread.sh\n",
            " mode change 100755 => 100644 onpolicy/scripts/train_smacv2_scripts/train_protoss_10v10.sh\n",
            " mode change 100755 => 100644 onpolicy/scripts/train_smacv2_scripts/train_protoss_10v11.sh\n",
            " mode change 100755 => 100644 onpolicy/scripts/train_smacv2_scripts/train_protoss_20v20.sh\n",
            " mode change 100755 => 100644 onpolicy/scripts/train_smacv2_scripts/train_protoss_20v23.sh\n",
            " mode change 100755 => 100644 onpolicy/scripts/train_smacv2_scripts/train_protoss_5v5.sh\n",
            " mode change 100755 => 100644 onpolicy/scripts/train_smacv2_scripts/train_terran_10v10.sh\n",
            " mode change 100755 => 100644 onpolicy/scripts/train_smacv2_scripts/train_terran_10v11.sh\n",
            " mode change 100755 => 100644 onpolicy/scripts/train_smacv2_scripts/train_terran_20v20.sh\n",
            " mode change 100755 => 100644 onpolicy/scripts/train_smacv2_scripts/train_terran_20v23.sh\n",
            " mode change 100755 => 100644 onpolicy/scripts/train_smacv2_scripts/train_terran_5v5.sh\n",
            " mode change 100755 => 100644 onpolicy/scripts/train_smacv2_scripts/train_zerg_10v10.sh\n",
            " mode change 100755 => 100644 onpolicy/scripts/train_smacv2_scripts/train_zerg_10v11.sh\n",
            " mode change 100755 => 100644 onpolicy/scripts/train_smacv2_scripts/train_zerg_20v20.sh\n",
            " mode change 100755 => 100644 onpolicy/scripts/train_smacv2_scripts/train_zerg_20v23.sh\n",
            " mode change 100755 => 100644 onpolicy/scripts/train_smacv2_scripts/train_zerg_5v5.sh\n",
            "Enumerating objects: 47, done.\n",
            "Counting objects: 100% (47/47), done.\n",
            "Delta compression using up to 2 threads\n",
            "Compressing objects: 100% (29/29), done.\n",
            "Writing objects: 100% (29/29), 740.64 KiB | 7.71 MiB/s, done.\n",
            "Total 29 (delta 17), reused 0 (delta 0), pack-reused 0\n",
            "remote: Resolving deltas: 100% (17/17), completed with 14 local objects.\u001b[K\n",
            "To https://github.com/juji-lau/on-policy.git\n",
            "   b4bae27..c0d345e  main -> main\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "9I3m1LmetoIc"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}